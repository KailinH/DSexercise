"""
Source: https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/courseware/78a8f19e5e54432a938ae62dc0246780/ac54e7a579bd44258ab0b446ac8091c4/
Detect vandalsim on Wikipedia using bag of words model, specific knowledge and metada 
by training decision trees models and see how the accuracy improves along the way.
"""
rm(list=ls(all=TRUE))
# Read in the data
wiki = read.csv("/Users/kailinh/Documents/analytics_edge/wiki.csv", stringsAsFactors=FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
length(stopwords("english"))

#Create corpus for words added
library(NLP)
library(tm)
library(SnowballC)
corpusAdded = Corpus(VectorSource(wiki$Added)) # create corpus
corpusAdded
corpusAdded[[1]]
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english")) # remove stop words
corpusAdded = tm_map(corpusAdded, stemDocument) # stem the words
dtmAdded = DocumentTermMatrix(corpusAdded) # create matrix
dtmAdded

#Filter out sparse terms
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded

# Convert to a data frame
wordsAdded = as.data.frame(as.matrix(sparseAdded))
summary(wordsAdded)
colnames(wordsAdded) = paste("A", colnames(wordsAdded))


# Create a words removed bag of words model

corpusRemoved = Corpus(VectorSource(wiki$Removed)) # create corpus
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english")) # remove stop words
corpusRemoved = tm_map(corpusRemoved, stemDocument) # stem the words
dtmRemoved = DocumentTermMatrix(corpusRemoved) # create matrix
dtmRemoved

#Filter out sparse terms
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved
rowSums(as.matrix(dtmRemoved))
# Convert to a data frame
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
summary(wordsRemoved)
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))

# Combine two dataframes
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal

# Split the data
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
trainSparse = subset(wikiWords, split==TRUE)
testSparse = subset(wikiWords, split==FALSE)
# Baseline accuracy
table(testSparse$Vandal)
618/(618+545)

# Build and evaluate a CART model
library(rpart)
library(rpart.plot)
wikiCART = rpart(Vandal ~ ., data=trainSparse, method="class")
prp(wikiCART)

# Evaluate the performance of the model
predictCART = predict(wikiCART, newdata=testSparse, type="class")
table(testSparse$Vandal, predictCART)
630/1163
# check if the model overfits
trainCART = predict(wikiCART, type="class")
table(trainSparse$Vandal, trainCART)
1476/(1476+1237) # similar to test set accuracy, so it doesn't overfit

# Use specific knowlege to improve the model
# adding URL in wordAdded is usually a sign
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
table(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
wikiCART2 = rpart(Vandal ~ ., data=wikiTrain2, method="class")
prp(wikiCART)
predictCART2 = predict(wikiCART2, newdata=wikiTest2, type="class")
table(wikiTest2$Vandal, predictCART2)
(609+57)/(609+57+9+488)

# Add word counts
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
wikiTrain3 = subset(wikiWords2, split==TRUE)
wikiTest3 = subset(wikiWords2, split==FALSE)
wikiCART3 = rpart(Vandal ~ ., data=wikiTrain3, method="class")
predictCART3 = predict(wikiCART3, newdata=wikiTest3, type="class")
table(wikiTest3$Vandal, predictCART3)
(514+248)/(514+248+104+297)

# Use metadata
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain4 = subset(wikiWords3, split==TRUE)
wikiTest4 = subset(wikiWords3, split==FALSE)
wikiCART4 = rpart(Vandal ~ ., data=wikiTrain4, method="class")
predictCART4 = predict(wikiCART4, newdata=wikiTest4, type="class")
table(wikiTest4$Vandal, predictCART4)
(595+241)/(595+241+23+304)
prp(wikiCART4) # only three splits, so the model is not more complicated by using independent variable
