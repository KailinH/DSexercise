"""
Use regression tree and logistic regression to understand why peopele vote in R
Source: https://courses.edx.org/courses/course-v1:MITx+15.071x_3+1T2016/courseware/3372864201764d6d9f63931920e5152e/ab08d73980f046479d3bcd105a55b0c2/
"""
rm(list=ls(all=TRUE))
# Read in the data
gerber = read.csv("/Users/kailinh/Documents/analytics_edge/gerber.csv")
str(gerber)

# get a general sense of data by looking at the percentages
table(gerber$voting)
108696/(108696+235388) #percentage of people who vote 0.315
table(gerber$voting, gerber$civicduty) 
12021/(26197+12021) #0.314
96675/(96675+209191) # this way gives you same result as tapple( , ,mean)
table(gerber$voting, gerber$hawthorne)
12316/(12316+25888) #0.322
table(gerber$voting, gerber$self)
13191/(13191+25027) #0.345
table(gerber$voting, gerber$neighbors)
14438/(14438+23763) #0.378
#another way to get the answer
tapply(gerber$voting, gerber$civicduty, mean)
tapply(gerber$voting, gerber$hawthorne, mean)
tapply(gerber$voting, gerber$self, mean)
tapply(gerber$voting, gerber$neighbors, mean)

# Exploration and logistic regression
gerberLog = glm(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, family=binomial)
summary(gerberLog)

gerberLog2 = glm(voting ~ sex + control, data=gerber, family=binomial)
summary(gerberLog2)
# Accuracy of logistic regression
predictTrain = predict(gerberLog, type="response")
table(gerber$voting, predictTrain > 0.3)
(134513+51966)/(134513+100875+56730+51966)  #overall accuracy
tapply(predictTrain, gerber$voting, mean)
table(gerber$voting, predictTrain > 0.5) #another accuracy
235388/(235388+108696)
# compute AUC for logistic regression
ROCRpredTest = prediction(predictTrain, gerber$voting)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values) #compute AUC
auc # about 0.53, as good as random guessing, poor model

# Regression tree
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(CARTmodel) #only one node, no splits, none of the variables make a big enough effect
# force build the tree by setting cp = 0.0
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(CARTmodel2)

# just the control group
CARTmodel3 = rpart(voting ~ control, data=gerber, cp=0.0)
CARTmodel4 = rpart(voting ~ control + sex, data=gerber, cp=0.0)
prp(CARTmodel3, digits=6)
prp(CARTmodel4, digits=6)
#abs(Control Prediction - Non-Control Prediction)

Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(gerberLog2, newdata=Possibilities, type="response") # calculate the percentage voting for four possibilities
abs(0.290456 - 0.2908065) #difference of predicted probabilities for (women,control) between two models 

# test a new regression model
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial") # add a combination of variables
summary(LogModel2)
predict(LogModel2, newdata=Possibilities, type="response")
abs(0.2904558 - 0.290456) # now the difference between logistic model and regression tree is very small
